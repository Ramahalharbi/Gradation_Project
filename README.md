# Gradation_Project

# 🎧 HearMeWell: Emotion Signal Detection and Speech Recognition

**HearMeWell** is an AI-driven project that combines advanced deep learning models for **emotion signal detection** and **speech recognition**, tailored specifically for **Saudi Arabic voice messages**. The system is designed to enhance communication for individuals with hearing impairments by analyzing emotional tones in speech signals and converting spoken Arabic to readable text.

---

## 🌟 Features

- **🎙️ Emotion Detection**  
  Classifies emotions from Arabic voice signals into five categories:  
  `Happy`, `Sad`, `Angry`, `Disgust`, and `Calm`.

- **🗣️ Speech-to-Text Transcription**  
  Converts Saudi Arabic speech to text using **OpenAI's Whisper** model.

- **🔬 Dual Deep Learning Approaches**  
  1. **Numerical-Based Modeling**:  
     Extracts features like **MFCCs**, **Zero Crossing Rate (ZCR)**, and **chroma** from audio signals.  
  2. **Image-Based Modeling**:  
     Uses **Mel-Spectrograms** processed with **CNN-BiLSTM** and other neural network architectures.

- **📱 Android Application**  
  A fully functional mobile app interface to test real-time audio, view emotional output, and speech transcription.

---

## 🏆 Awards

🥇 **1st Place Winner – Best AI Project**  
📍 Digital Innovation and Entrepreneurship Forum, University of Jeddah

---

## 🎬 Watch Demo Video

https://github.com/user-attachments/assets/5c5bc106-3339-483a-9e03-7d08ab2a7f31




## 📷 App Preview


![HearMeWell_Image](https://github.com/user-attachments/assets/be225f49-5d91-4e62-b3ec-1132e293c25b)




---

## 🧪 Technologies Used

- **Language & Frameworks**: Python, Java (Android), TensorFlow, Keras  
- **Speech Recognition**: [OpenAI Whisper](https://github.com/openai/whisper)  
- **Signal Processing**: MFCC, ZCR, Chroma, Librosa  
- **Model Types**: CNN, BiLSTM, Hybrid  
- **Mobile UI**: Android Studio, Java  
- **Visualization**: Matplotlib, Mel-spectrograms

---

## 🧠 How It Works

1. The user sends an Arabic voice message.
2. The app processes the **audio signal**, extracts features, and generates spectrograms.
3. Two trained models classify the **emotion** and transcribe the **speech**.
4. The user receives a labeled emotion and written text of the message.

---

## 🤝 Contact

If you're interested in collaborating, learning more, or providing feedback, feel free to open an issue or contact us via email:

📧 **alharbiramah@gmail.com**

---

# Gradation_Project

# ğŸ§ HearMeWell: Emotion Signal Detection and Speech Recognition

**HearMeWell** is an AI-driven project that combines advanced deep learning models for **emotion signal detection** and **speech recognition**, tailored specifically for **Saudi Arabic voice messages**. The system is designed to enhance communication for individuals with hearing impairments by analyzing emotional tones in speech signals and converting spoken Arabic to readable text.

---

## ğŸŒŸ Features

- **ğŸ™ï¸ Emotion Detection**  
  Classifies emotions from Arabic voice signals into five categories:  
  `Happy`, `Sad`, `Angry`, `Disgust`, and `Calm`.

- **ğŸ—£ï¸ Speech-to-Text Transcription**  
  Converts Saudi Arabic speech to text using **OpenAI's Whisper** model.

- **ğŸ”¬ Dual Deep Learning Approaches**  
  1. **Numerical-Based Modeling**:  
     Extracts features like **MFCCs**, **Zero Crossing Rate (ZCR)**, and **chroma** from audio signals.  
  2. **Image-Based Modeling**:  
     Uses **Mel-Spectrograms** processed with **CNN-BiLSTM** and other neural network architectures.

- **ğŸ“± Android Application**  
  A fully functional mobile app interface to test real-time audio, view emotional output, and speech transcription.

---

## ğŸ† Awards

ğŸ¥‡ **1st Place Winner â€“ Best AI Project**  
ğŸ“ Digital Innovation and Entrepreneurship Forum, University of Jeddah

---

## ğŸ¬ Watch Demo Video

https://github.com/user-attachments/assets/5c5bc106-3339-483a-9e03-7d08ab2a7f31




## ğŸ“· App Preview


![HearMeWell_Image](https://github.com/user-attachments/assets/be225f49-5d91-4e62-b3ec-1132e293c25b)




---

## ğŸ§ª Technologies Used

- **Language & Frameworks**: Python, Java (Android), TensorFlow, Keras  
- **Speech Recognition**: [OpenAI Whisper](https://github.com/openai/whisper)  
- **Signal Processing**: MFCC, ZCR, Chroma, Librosa  
- **Model Types**: CNN, BiLSTM, Hybrid  
- **Mobile UI**: Android Studio, Java  
- **Visualization**: Matplotlib, Mel-spectrograms

---

## ğŸ§  How It Works

1. The user sends an Arabic voice message.
2. The app processes the **audio signal**, extracts features, and generates spectrograms.
3. Two trained models classify the **emotion** and transcribe the **speech**.
4. The user receives a labeled emotion and written text of the message.

---

## ğŸ¤ Contact

If you're interested in collaborating, learning more, or providing feedback, feel free to open an issue or contact us via email:

ğŸ“§ **alharbiramah@gmail.com**

---

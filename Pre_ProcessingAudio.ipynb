{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.inference.enhancement import SpectralMaskEnhancement\n",
    "import numpy as np \n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import glob\n",
    "import shutil\n",
    "from transformers import pipeline\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained model enhancing speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input folder and output folder\n",
    "input_folder = 'batch_1/*.wav'\n",
    "output_enhanced_folder = 'enhanced_audios/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_enhanced_folder, exist_ok=True)\n",
    "\n",
    "# Load the enhancement model\n",
    "enhance_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"speechbrain/metricgan-plus-voicebank\",\n",
    "    savedir=\"pretrained_models/metricgan-plus-voicebank\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each audio file in the input folder\n",
    "for idx, audio_file in enumerate(glob.glob(input_folder)):\n",
    "    # Load and add fake batch dimension\n",
    "    noisy = enhance_model.load_audio(audio_file).unsqueeze(0)\n",
    "\n",
    "    # Add relative length tensor and enhance the audio\n",
    "    enhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\n",
    "\n",
    "    # Save the enhanced signal on disk with a unique filename\n",
    "    enhanced_file_path = os.path.join(output_enhanced_folder, f'EnhancedSpeech_{idx+1}.wav')\n",
    "    torchaudio.save(enhanced_file_path, enhanced.cpu(), 16000)\n",
    "\n",
    "# Optionally, print a message indicating completion\n",
    "print(\"Enhancement completed for all audio files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library to reduse noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output folders\n",
    "input_enhanced_folder = 'enhanced_audios/'\n",
    "output_cleaned_folder = 'CleanedAudios/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_cleaned_folder, exist_ok=True)\n",
    "\n",
    "# Process each enhanced audio file in the input folder\n",
    "for enhanced_audio_file in glob.glob(os.path.join(input_enhanced_folder, '*.wav')):\n",
    "    # Load the enhanced audio\n",
    "    audio_data, sr = librosa.load(enhanced_audio_file)\n",
    "\n",
    "    # Reduce noise\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio_data, sr=sr)\n",
    "\n",
    "    # Save the cleaned audio in the CleanedAudios folder\n",
    "    cleaned_file_path = os.path.join(output_cleaned_folder, os.path.basename(enhanced_audio_file))\n",
    "    sf.write(cleaned_file_path, reduced_noise_audio, sr)\n",
    "\n",
    "# Optionally, print a message indicating completion\n",
    "print(\"Noise reduction completed for all enhanced audio files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained model for extracting the speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_actual_token_here' with your Hugging Face access token\n",
    "HUGGINGFACE_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for segmentation from Hugging Face\n",
    "model = Model.from_pretrained(\n",
    "  \"pyannote/segmentation-3.0\",  # Ensure you've accepted the model's user conditions\n",
    "  use_auth_token=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the voice activity detection pipeline using the model\n",
    "pipeline = VoiceActivityDetection(segmentation=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the voice activity detection\n",
    "HYPER_PARAMETERS = {\n",
    "    \"min_duration_on\": 1.0,  # Minimum duration of speech (in seconds)\n",
    "    \"min_duration_off\": 1.5,  # Minimum duration of silence (in seconds)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the pipeline with the defined hyperparameters\n",
    "pipeline.instantiate(HYPER_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply VAD and split audio based on speech segments\n",
    "def split_and_extract_speech_with_vad(input_audio_file, output_dir, file_index):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Run VAD on the input audio file\n",
    "    vad = pipeline(input_audio_file)\n",
    "\n",
    "    # Load the original audio file\n",
    "    audio, sr = librosa.load(input_audio_file, sr=None)\n",
    "\n",
    "    # Process each speech segment\n",
    "    for i, segment in enumerate(vad.itersegments()):\n",
    "        start_time = int(segment.start * sr)  # Convert from seconds to samples\n",
    "        end_time = int(segment.end * sr)\n",
    "\n",
    "        # Extract the speech segment from the original audio\n",
    "        speech_audio = audio[start_time:end_time]\n",
    "\n",
    "        # If the segment has some valid length, save it as a separate file\n",
    "        if len(speech_audio) > 0:\n",
    "            output_file_path = os.path.join(output_dir, f\"speech_segment_{file_index}_{i + 1}.wav\")\n",
    "            sf.write(output_file_path, speech_audio, sr)\n",
    "            print(f\"Saved: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process all files in the CleanedAudios folder\n",
    "def process_cleaned_audios(input_folder, output_folder):\n",
    "    # Loop through all audio files in the input folder\n",
    "    for idx, audio_file in enumerate(glob.glob(os.path.join(input_folder, '*.wav'))):\n",
    "        # Apply VAD and extract speech segments from each file\n",
    "        split_and_extract_speech_with_vad(audio_file, output_folder, idx + 1)\n",
    "\n",
    "# Define the input folder containing cleaned audios and output folder for speech segments\n",
    "input_cleaned_folder = \"Batch4Audio\"  # Folder with cleaned audio files\n",
    "output_speech_segments_folder = 'Segments_4/'  # Folder to save all speech segments\n",
    "\n",
    "# Ensure the output folder for speech segments exists\n",
    "os.makedirs(output_speech_segments_folder, exist_ok=True)\n",
    "\n",
    "# Process all cleaned audio files and extract speech segments\n",
    "if __name__ == \"__main__\":\n",
    "    process_cleaned_audios(input_cleaned_folder, output_speech_segments_folder)\n",
    "\n",
    "print(\"Speech extraction completed for all cleaned audio files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset labeling with emotions using pre-training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_classification_pipeline = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing batch folders\n",
    "processed_audio_folder = \"C:\\\\Users\\\\rkhm3\\\\Desktop\\\\PerfectCLeaning\\\\AllAudioData\"\n",
    "# Path to the folder where emotion-labeled folders will be created\n",
    "output_folder = \"Emotion_Classified_Audio\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter to rename files\n",
    "audio_counter = 1\n",
    "\n",
    "# Iterate over each batch folder\n",
    "for batch_folder in os.listdir(processed_audio_folder):\n",
    "    batch_folder_path = os.path.join(processed_audio_folder, batch_folder)\n",
    "    \n",
    "    # Skip non-folder files\n",
    "    if not os.path.isdir(batch_folder_path):\n",
    "        continue\n",
    "\n",
    "    # Iterate over all audio files in the batch folder\n",
    "    for file_name in os.listdir(batch_folder_path):\n",
    "        audio_file_path = os.path.join(batch_folder_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Use the pipeline to classify the audio file\n",
    "            result = audio_classification_pipeline(audio_file_path)\n",
    "            emotion = result[0]['label']\n",
    "            \n",
    "            # Define the folder for this emotion\n",
    "            emotion_folder = os.path.join(output_folder, emotion)\n",
    "            os.makedirs(emotion_folder, exist_ok=True)\n",
    "            \n",
    "            # Rename and move the audio file to the emotion folder\n",
    "            new_file_name = f\"Audio_{audio_counter}_{emotion}.wav\"\n",
    "            new_file_path = os.path.join(emotion_folder, new_file_name)\n",
    "            \n",
    "            # Copy the file to the emotion folder with the new name\n",
    "            shutil.copy(audio_file_path, new_file_path)\n",
    "            print(f\"Saved: {new_file_name}\")\n",
    "            # Increment the counter\n",
    "            audio_counter += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_file_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(\"Audio files have been classified, renamed, and saved by emotion.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define dataset paths\n",
    "original_dataset_path = r\"C:\\Users\\rkhm3\\Desktop\\HearWell\\Emotion_Classified_Audio\" # Path to your dataset folders\n",
    "balanced_dataset_path = \"Balanced_Emotion_Audio\"  # Path to save the balanced dataset\n",
    "target_samples = 14000  # Target number of samples per emotion\n",
    "\n",
    "# Target emotions (folders to balance)\n",
    "target_emotions = ['angry', 'calm', 'disgust', 'happy', 'sad']\n",
    "\n",
    "# Create the balanced dataset directory\n",
    "os.makedirs(balanced_dataset_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over each emotion folder\n",
    "for emotion in target_emotions:\n",
    "    emotion_folder = os.path.join(original_dataset_path, emotion)\n",
    "    if not os.path.exists(emotion_folder):\n",
    "        print(f\"Emotion folder '{emotion}' does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Get list of all files in the emotion folder\n",
    "    files = [f for f in os.listdir(emotion_folder) if os.path.isfile(os.path.join(emotion_folder, f))]\n",
    "\n",
    "    # Randomly sample files if there are more than target_samples\n",
    "    if len(files) > target_samples:\n",
    "        sampled_files = random.sample(files, target_samples)\n",
    "    else:\n",
    "        print(f\"Not enough files in '{emotion}'. Using all {len(files)} files.\")\n",
    "        sampled_files = files\n",
    "\n",
    "    # Create the emotion folder in the balanced dataset directory\n",
    "    balanced_emotion_folder = os.path.join(balanced_dataset_path, emotion)\n",
    "    os.makedirs(balanced_emotion_folder, exist_ok=True)\n",
    "\n",
    "    # Copy sampled files to the balanced dataset directory\n",
    "    for file in sampled_files:\n",
    "        src_path = os.path.join(emotion_folder, file)\n",
    "        dest_path = os.path.join(balanced_emotion_folder, file)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "    print(f\"Balanced dataset created for emotion '{emotion}' with {len(sampled_files)} files.\")\n",
    "\n",
    "print(\"Balanced dataset creation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
